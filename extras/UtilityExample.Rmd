---
title: "Utility Example"
author: "Nina Zumel"
date: "9/30/2020"
output: github_document
---

This is an example of how to use [`sigr::model_utility()`](https://winvector.github.io/sigr/reference/model_utility.html) to estimate model utility and pick model thresholds for classificaiton problems.

Define our data generator function, which we will use to simply *simulate* the kind of results our data scientist is seeing.

```{r}
set.seed(2020)

y_example <- function(n, prevalence = 0.5) {
  data.frame(
    y = sample(
      c(TRUE, FALSE), 
      size = n, 
      replace = TRUE,
      prob = c(prevalence, 1 - prevalence))
  )
}

beta_variable <- function(
  d, 
  shape1_pos, shape2_pos, 
  shape1_neg, shape2_neg) {
  score <- numeric(nrow(d))
  score[d$y] <- rbeta(sum(d$y), shape1 = shape1_pos, shape2 = shape2_pos)
  score[!d$y] <- rbeta(sum(!d$y), shape1 = shape1_neg, shape2 = shape2_neg)
  score
}
```

Then generate some synthetic example data. This is a population where true positives are relatively rare (1% of the population).

```{r}
d <- y_example(10000, prevalence = 0.01)

d$predicted_probability <- beta_variable(
  d,
  shape1_pos = 2, 
  shape2_pos = 1,
  shape1_neg = 6, # 5
  shape2_neg = 6)

library(WVPlots)
DoubleDensityPlot(d, "predicted_probability", "y",
                  title = "Model score distribution by true outcome")

```


## Picking a threshold based on model performance

Let's look at precision (the probability that a sample scored as "true" really is true) and recall as a function of threshold.

```{r}
ThresholdPlot(d, "predicted_probability", "y",
              title = "Precision and Recall as a function of threshold",
              metrics = c("precision", "recall"))
```

In the abstract, we think we want to use a threshold of at least 0.87 or so to get decent precision (above 75%). This would only recover us at most 25% of the actual positives. Is that good enough for our business needs?

## Picking a threshold based on model *utility*

The way to decide if a possible threshold meets business needs is to attach utilities to the decisions -- right and wrong -- that the model will make. Let's imagine that this is a sales application, and we are trying to decide which prospects to target, and our model predicts probability of conversion.

Every contact we make costs \$5, and every successful sale brings in \$100 in revenue. We will also add a small penalty for every missed prospect (one penny), and a small reward for every prospect we correctly ignore (again, one penny). Let's add those costs to our score matrix. (You can leave the last two values at zero, if you prefer).

```{r}
d$true_positive_value <- 100 - 5   # revenue - cost
d$false_positive_value <- -5       # the cost of a call
d$true_negative_value <-  0.01      # a small reward for getting them right
d$false_negative_value <- -0.01        # a small penalty for having missed them
```

The `sigr::model_utility()` function then calculates all the costs for various thresholds.

```{r}
library(sigr)
values <- model_utility(d, 
                        model_name = 'predicted_probability', 
                        outcome_name = 'y')
head(values)
```

Each row of `values` returns the appropriate counts and values for a classifier rule that labels cases as TRUE when `predicted_probability >= threshold`. Now we can determine the total value returned by the model on our evaluation set, as a function of threshold.

```{r}
library(ggplot2)

vhigh <- subset(values, threshold >= 0.5) # just look at thresholds > 0.5
p <- ggplot(vhigh, aes(x = threshold, y = total_value)) + 
  geom_line() + 
  ggtitle("Estimated model utility as a function of threshold")

# find the maximum utility
max_ix <- which.max(vhigh$total_value)
best_threshold <- vhigh$threshold[max_ix]

p + geom_vline(xintercept = best_threshold, linetype=3)

# print out some information about the optimum
(best_info <- vhigh[max_ix, c("threshold", "count_taken", "fraction_taken", "total_value")])

```

This suggests that with this model we should use threshold `r format(best_threshold, digits=2)`, which translates to calling the top `r 100*vhigh$fraction_taken[max_ix]`% of the prospects. 

We can compare this to the best possible performance on this data (calling all of the successful prospects, and only them), simply by positing a "wizard model" that scores true cases as 1.0 and false cases as 0.0

```{r}
# add a column for the wizard model
d$wizard <- with(d, ifelse(y, 1.0, 0.0))

# calculate the wizard's model utilities
wizard_values <- model_utility(d, 
                               model_name = 'wizard',
                               outcome_name = 'y')
wizard_values[, c("threshold", "count_taken", "fraction_taken", "total_value")]

# amount of potential value in this population
potential = wizard_values$total_value[2]

# what we realized
realized = best_info$total_value

# fraction realized
frac_realized = realized/potential

```

The `threshold = 0.5` row tells us that the potential value of this population sample is \$`r format(potential)`, of which our model realized `r format(100*frac_realized, digits=3)`%.

### Advanced: Variable Utilities

In the above example, we assigned constant utilities and costs to the data set. Here, instead of assuming a fixed revenue of \$100 dollars on conversions, we'll assume that revenue varies by prospect (with an average of \$100).

```{r}
# replace the true positive value
d$true_positive_value <- rnorm(nrow(d), mean = 100, sd = 25) - 5   # revenue - cost

# recalculate the values and potential
values <- model_utility(d, 
                        model_name = 'predicted_probability', 
                        outcome_name = 'y')

wizard_values <- model_utility(d, 
                               model_name = 'wizard',
                               outcome_name = 'y')
```

Here we replot the utility curve.

```{r}
vhigh <- subset(values, threshold >= 0.5) # just look at thresholds > 0.5
p <- ggplot(vhigh, aes(x = threshold, y = total_value)) + 
  geom_line() + 
  ggtitle("Estimated model utility as a function of threshold, variable utility case")

# find the maximum utility
max_ix <- which.max(vhigh$total_value)
best_threshold <- vhigh$threshold[max_ix]

p + geom_vline(xintercept = best_threshold, linetype=3)

# print out some information about the optimum
(best_info <- vhigh[max_ix, c("threshold", "count_taken", "fraction_taken", "total_value")])

# compare to potential
wizard_values[2, c("threshold", "count_taken", "fraction_taken", "total_value")]

```
