---
title: "Utility Example"
author: "Nina Zumel"
date: "9/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Define our data generator function, which we will use to simply *simulate* the kind of results our data scientist is seeing.

```{r}
set.seed(2020)

y_example <- function(n, prevalence = 0.5) {
  data.frame(
    y = sample(
      c(TRUE, FALSE), 
      size = n, 
      replace = TRUE,
      prob = c(prevalence, 1 - prevalence))
  )
}

beta_variable <- function(
  d, 
  shape1_pos, shape2_pos, 
  shape1_neg, shape2_neg) {
  score <- numeric(nrow(d))
  score[d$y] <- rbeta(sum(d$y), shape1 = shape1_pos, shape2 = shape2_pos)
  score[!d$y] <- rbeta(sum(!d$y), shape1 = shape1_neg, shape2 = shape2_neg)
  score
}
```

Then generate some synthetic example data. This is a population where true positives are relatively rare (1% of the population).

```{r}
d <- y_example(10000, prevalence = 0.01)

d$predicted_probability <- beta_variable(
  d,
  shape1_pos = 2, 
  shape2_pos = 1,
  shape1_neg = 6, # 5
  shape2_neg = 6)

library(WVPlots)
DoubleDensityPlot(d, "predicted_probability", "y",
                  title = "Model score distribution by true outcome")

```

Let's look at precision (the probability that a sample scored as "true" really is true) and recall as a function of threshold.

```{r}
ThresholdPlot(d, "predicted_probability", "y",
              title = "Precision and Recall as a function of threshold",
              metrics = c("precision", "recall"))
```

In the abstract, we would want to use a threshold of at least 0.87 or so to get decent precision (above 75%). This would only recover us at most 25% of the actual positives. Is that good enough for our business needs?

The way to decide that is to attach utilities to positives and negatives. Let's imagine that this is a sales application, and we are trying to decide which prospects to target, and our model predicts probability of conversion.

Every contact we make costs \$5, and every successful sale brings in \$100 in revenue. We will also add a small penalty for every missed prospect (one penny), and a small reward for every prospect we correctly ignore (again, one penny). Let's add those costs to our score matrix.

```{r}
d$true_positive_value = 100 - 5   # revenue - cost
d$false_positive_value = -5       # the cost of a call
d$true_negative_value = 0.01      # a small reward for getting them right
d$false_negative_value = 0        # a small penalty for having missed them
```

The `sigr::model_utility()` function then calculates all the costs for various thresholds.

```{r}
library(sigr)
values <- model_utility(d, 
                        model_name = 'predicted_probability', 
                        outcome_name = 'y')
head(values)
```

We can determine the total value returned by the model on our evaluation set, as a function of threshold.

```{r}
library(ggplot2)

vhigh = subset(values, threshold >= 0.5) # just look at thresholds > 0.5
p = ggplot(vhigh, aes(x = threshold, y = total_value)) + 
  geom_line() + 
  ggtitle("Estimated model utility as a function of threshold")

# find the maximum utility
max_ix = which.max(vhigh$total_value)

p + geom_vline(xintercept = vhigh$threshold[max_ix], linetype=3)

# print out some information about the optimum
vhigh[max_ix, c("threshold", "count_taken", "fraction_taken", "total_value")]

# for comparison, what's the total value of this

```

This suggests that with this model we should use threshold `r vhigh$threshold[max_ix]`, which translates to calling the top `r 100*vhigh$fraction_taken[max_ix]`% of the prospects

