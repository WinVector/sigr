---
title: "Squeezing the Most Utility from Your Models"
output: github_document
---

In a [previous article](https://win-vector.com/2020/08/07/dont-use-classification-rules-for-classification-problems/) we discussed why it's a good idea to prefer probability models to "hard" classification models, and why you should delay setting "hard" classification rules as long as possible. But decisions have to be made, and eventually you will have to set that threshold. How do you do it?

A good threshold balances classifier precision/recall or sensitivity/specificity in a way that best meets the project or business needs. One way to quantify and think about this balance is the notion of *model utility*, which maps the performance of a model to some notion of the value achieved by that performance. In this article, we demonstrate the use of [`sigr::model_utility()`](https://winvector.github.io/sigr/reference/model_utility.html) to estimate model utility and pick model thresholds for classification problems.

## The Example Problem

Let's imagine that we have a sales application, and we are trying to decide which prospects to target. We want to build a model that predicts the probability of conversion, based on certain prospect characteristics.

For our example, suppose we have a population where true positives are relatively rare (about 1% of the population), and we have trained a model that returns predictions like this on an evaluation set, `d`:

```{r echo=FALSE}
#
# data generator functions
#
set.seed(2020)

y_example <- function(n, prevalence = 0.5) {
  data.frame(
    y = sample(
      c(TRUE, FALSE), 
      size = n, 
      replace = TRUE,
      prob = c(prevalence, 1 - prevalence))
  )
}

beta_variable <- function(
  d, 
  shape1_pos, shape2_pos, 
  shape1_neg, shape2_neg) {
  score <- numeric(nrow(d))
  score[d$y] <- rbeta(sum(d$y), shape1 = shape1_pos, shape2 = shape2_pos)
  score[!d$y] <- rbeta(sum(!d$y), shape1 = shape1_neg, shape2 = shape2_neg)
  score
}

#
# generate the example
#

d <- y_example(10000, prevalence = 0.01)

d$predicted_probability <- beta_variable(
  d,
  shape1_pos = 2, 
  shape2_pos = 1,
  shape1_neg = 6, # 5
  shape2_neg = 6)

# change the column name to better match the example
colnames(d) <- c("converted", "predicted_probability")

```

```{r message=FALSE}
library(WVPlots)
DoubleDensityPlot(d, "predicted_probability", "converted",
                  title = "Model score distribution by true outcome")
```

(For the data generation code, see [the source code for this article](https://github.com/WinVector/sigr/blob/main/extras/UtilityExample.Rmd)).

We want to contact prospects with a higher probability of converting; that is, prospects who score above a certain threshold.
How how do we set that threshold?

## Picking a threshold based on model performance

We might try picking the threshold by looking at model performance. Let's look at precision (the probability that a sample scored as "true" really is true) and recall (the fraction of conversions found) as a function of threshold.

```{r warning=FALSE}
ThresholdPlot(d, "predicted_probability", "converted",
              title = "Precision and Recall as a function of threshold",
              metrics = c("precision", "recall"))
```

In the abstract, we think we want to use a threshold of at least 0.87 or so to get decent precision (above 75%). This would only recover us at most 25% of the actual positives. Is that good enough for our business needs? This can be hard to tell, as the business needs are likely in dollars, rather than in false positives/false negatives, and so on.

## Picking a threshold based on model *utility*

The way to decide if a possible threshold meets business needs is to attach utilities to the decisions -- right and wrong -- that the model will make. To do this, we need to assign costs and rewards to contacting (or not contacting) a prospect. 

Suppose every contact we make costs \$5, and every successful conversion brings in \$100 in net revenue. For demonstration purposes, we will also add a small penalty for every missed prospect (one penny), and a small reward for every prospect we correctly ignore (again, one penny). Let's add those costs to our model frame. (You can leave the last two values at zero, if you prefer).

```{r}
d$true_positive_value <- 100 - 5   # net revenue - cost
d$false_positive_value <- -5       # the cost of a call
d$true_negative_value <-  0.01     # a small reward for getting them right
d$false_negative_value <- -0.01    # a small penalty for having missed them
```

As we vary the decision threshold, we vary the number of prospects contacted, and the number of successful conversions. We can use the costs and rewards above to calculate the total value (or the *total utility*) realized by the decision process over the evaluation set. The threshold that realizes the highest utility is the "best" threshold to use with a given model (for now, we ignore also modeling uncertainty).

The `sigr::model_utility()` function can calculate all the costs for various thresholds.

```{r}
library(sigr)
values <- model_utility(d, 
                        model_name = 'predicted_probability', 
                        outcome_name = 'converted')
```

The `model_utility()` function returns a data frame with the following columns:

```{r echo=FALSE}
knitr::kable(head(values))
```

Each row of `values` returns the appropriate counts and values for a classifier rule that labels cases as TRUE when `predicted_probability >= threshold`. Now we can determine the total value returned by the model on our evaluation set, as a function of threshold.

```{r}
library(ggplot2)

vhigh <- subset(values, threshold >= 0.5) # just look at thresholds > 0.5
p <- ggplot(vhigh, aes(x = threshold, y = total_value)) + 
  geom_line() + 
  ggtitle("Estimated model utility as a function of threshold")

# find the maximum utility
max_ix <- which.max(vhigh$total_value)
best_threshold <- vhigh$threshold[max_ix]

p + geom_vline(xintercept = best_threshold, linetype=3)

# print out some information about the optimum
best_info <- vhigh[max_ix, c("threshold", "count_taken", 
                             "fraction_taken", "total_value")]
knitr::kable(best_info)

```

This suggests that with this model we should use threshold `r format(best_threshold, digits=2)`, which translates to calling the top `r 100*vhigh$fraction_taken[max_ix]`% of the prospects. 

We can compare this to the best possible performance on this data (contacting all of the successful prospects, and only them), simply by positing a "wizard model" that scores true cases as 1.0 and false cases as 0.0

```{r}
# add a column for the wizard model
d$wizard <- with(d, ifelse(converted, 1.0, 0.0))

# calculate the wizard's model utilities
wizard_values <- model_utility(d, 
                               model_name = 'wizard',
                               outcome_name = 'converted')
wizard_values[2, c("threshold", "count_taken", 
                  "fraction_taken", "total_value")]

# amount of potential value in this population
potential = wizard_values$total_value[2]

# what we realized
realized = best_info$total_value

# fraction realized
frac_realized = realized/potential

```

The `threshold = 0.5` row tells us that the potential value of this population sample is \$`r format(potential)`, of which our model realized \$`r format(realized)`, or `r format(100*frac_realized, digits=3)`%. This is easier and more reliable than just eyeballing a threshold from the abstract recall/precision graph.

### Advanced: Variable Utilities

In the above example, we assigned constant utilities and costs to the data set. Here, instead of assuming a fixed revenue of \$100 dollars on conversions, we'll assume that projected potential revenue varies by prospect (with an average of \$100).

```{r}
# replace the true positive value with a varying value
d$true_positive_value <- rnorm(nrow(d), mean = 100, sd = 25) - 5   # revenue - cost

# recalculate the values and potential
values <- model_utility(d, 
                        model_name = 'predicted_probability', 
                        outcome_name = 'converted')

wizard_values <- model_utility(d, 
                               model_name = 'wizard',
                               outcome_name = 'converted')
```

Here we replot the utility curve.

```{r}
vhigh <- subset(values, threshold >= 0.5) # just look at thresholds > 0.5
p <- ggplot(vhigh, aes(x = threshold, y = total_value)) + 
  geom_line() + 
  ggtitle("Estimated model utility as a function of threshold, variable utility case")

# find the maximum utility
max_ix <- which.max(vhigh$total_value)
best_threshold <- vhigh$threshold[max_ix]

p + geom_vline(xintercept = best_threshold, linetype=3)

# print out some information about the optimum
(best_info <- vhigh[max_ix, c("threshold", "count_taken", "fraction_taken", "total_value")])

# compare to potential
wizard_values[2, c("threshold", "count_taken", "fraction_taken", "total_value")]

```

### Reporting Uncertainty

Up until now, we have returned a point utility estimate. It's better to use a distribution, or at least an estimate of uncertainty. In our next note, we will deal with estimating and reporting uncertainty.

## Conclusion

We have shown how to select a classifier threshold directly from policy utility.
Thinking in terms of utility is simple,  because it's already the language of your business partners.

